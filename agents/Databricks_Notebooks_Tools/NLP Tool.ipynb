{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baa3e729-4683-4736-b9ff-cc133fbc48e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Job Notebook: property vs neighbors review issues (writes Delta outputs)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Inputs (widgets):\n",
    "#   delta_path:      Delta path of listings/similarity table\n",
    "#   out_base_path:   Base path to write Delta outputs (required)\n",
    "#   pid:             target property_id\n",
    "#   params_json:     JSON string of parameters (optional; overrides defaults)\n",
    "#\n",
    "# Outputs (Delta paths under out_base_path):\n",
    "#   /runs, /neighbors, /topics, /evidence\n",
    "#\n",
    "# Notebook exit value (JSON):\n",
    "#   {\"status\":\"ok\",\"run_id\":\"...\",\"property_id\":\"...\",\"written\":{...}} or {\"status\":\"error\",...}\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "\n",
    "import pandas as pd  # kept since your original script imports it (safe)\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Spark NLP (assumed available)\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import (\n",
    "    SentenceDetectorDLModel,\n",
    "    Tokenizer,\n",
    "    ViveknSentimentModel,\n",
    "    UniversalSentenceEncoder,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Widgets (Job parameters)\n",
    "# -----------------------------\n",
    "dbutils.widgets.text(\"delta_path\", \"/mnt/lab94290/cluster_19/airbnb_h3_simvector\")\n",
    "dbutils.widgets.text(\"out_base_path\", \"\")\n",
    "dbutils.widgets.text(\"pid\", \"\")\n",
    "dbutils.widgets.text(\"params_json\", \"\")\n",
    "\n",
    "DELTA_PATH = dbutils.widgets.get(\"delta_path\").strip()\n",
    "OUT_BASE_PATH = dbutils.widgets.get(\"out_base_path\").strip()\n",
    "PID = dbutils.widgets.get(\"pid\").strip()\n",
    "PARAMS_JSON = dbutils.widgets.get(\"params_json\").strip()\n",
    "\n",
    "if not DELTA_PATH:\n",
    "    raise ValueError(\"delta_path widget is required\")\n",
    "if not OUT_BASE_PATH:\n",
    "    raise ValueError(\"out_base_path widget is required (base Delta path for outputs)\")\n",
    "if not PID:\n",
    "    raise ValueError(\"pid widget is required\")\n",
    "\n",
    "# -----------------------------\n",
    "# Column names (adjust only if your schema differs)\n",
    "# -----------------------------\n",
    "ID_COL  = \"property_id\"\n",
    "REV_COL = \"reviews\"\n",
    "\n",
    "VEC_COL  = \"similarity_vec\"\n",
    "BAD_COL  = \"is_bad_vec\"\n",
    "NAN_COL  = \"has_nan\"\n",
    "NORM_COL = \"vec_norm\"\n",
    "\n",
    "# -----------------------------\n",
    "# Topics list\n",
    "# -----------------------------\n",
    "TOPICS = [\n",
    "    \"cleanliness and hygiene\",\n",
    "    \"maintenance and repairs\",\n",
    "    \"smell and air quality\",\n",
    "    \"noise and sound disturbances\",\n",
    "    \"privacy and quietness\",\n",
    "    \"safety and security\",\n",
    "    \"bed comfort and sleep quality\",\n",
    "    \"temperature and climate control\",\n",
    "    \"space and layout comfort\",\n",
    "    \"kitchen and cooking facilities\",\n",
    "    \"bathroom quality\",\n",
    "    \"wifi and internet quality\",\n",
    "    \"appliances and amenities availability\",\n",
    "    \"location and neighborhood\",\n",
    "    \"public transportation access\",\n",
    "    \"parking availability\",\n",
    "    \"host communication and responsiveness\",\n",
    "    \"check-in and check-out process\",\n",
    "    \"accuracy of listing description\",\n",
    "    \"value for money and pricing\",\n",
    "]\n",
    "\n",
    "# =========================================================\n",
    "# Helpers\n",
    "# =========================================================\n",
    "def _pick_geo_col(df: DataFrame) -> str:\n",
    "    if \"geo_bucket\" in df.columns:\n",
    "        return \"geo_bucket\"\n",
    "    if \"_geo_bucket_filled\" in df.columns:\n",
    "        return \"_geo_bucket_filled\"\n",
    "    raise ValueError(\"No geo bucket column found: expected 'geo_bucket' or '_geo_bucket_filled'.\")\n",
    "\n",
    "def filter_english_like(df: DataFrame, text_col: str = \"sentence_text\", non_ascii_ratio_max: float = 0.20) -> DataFrame:\n",
    "    non_ascii = F.length(F.col(text_col)) - F.length(F.regexp_replace(F.col(text_col), r\"[^\\x00-\\x7F]\", \"\"))\n",
    "    total = F.greatest(F.length(F.col(text_col)), F.lit(1))\n",
    "    non_ascii_ratio = non_ascii / total\n",
    "    has_latin = F.col(text_col).rlike(r\"[A-Za-z]\")\n",
    "    return df.filter(has_latin & (non_ascii_ratio <= F.lit(float(non_ascii_ratio_max))))\n",
    "\n",
    "def _safe_head1(df: DataFrame):\n",
    "    rows = df.take(1)\n",
    "    return rows[0] if rows else None\n",
    "\n",
    "# =========================================================\n",
    "# 1) Neighbors selection by cosine similarity in same geo bucket\n",
    "#    REQUIRED:\n",
    "#    - Identify target geo_bucket_res\n",
    "#    - Use column h3_{geo_bucket_res}\n",
    "#    - Filter candidates where h3_{geo_bucket_res} == bucket value\n",
    "# =========================================================\n",
    "def top_similar_properties_with_metadata(\n",
    "    df: DataFrame,\n",
    "    target_property_id,\n",
    "    *,\n",
    "    max_candidates: int = 50,\n",
    "    top_k: int = 20,\n",
    "    seed: int = 42,\n",
    "    oversample: float = 1.7,\n",
    ") -> Tuple[DataFrame, Dict[str, Any]]:\n",
    "    spark = df.sparkSession\n",
    "    bucket_val_col = _pick_geo_col(df)\n",
    "\n",
    "    if \"geo_bucket_res\" not in df.columns:\n",
    "        raise ValueError(\"Missing required column 'geo_bucket_res' in df.\")\n",
    "\n",
    "    target_df = (\n",
    "        df.filter(F.col(ID_COL).cast(\"string\") == F.lit(str(target_property_id)))\n",
    "          .select(\n",
    "              F.col(ID_COL).cast(\"string\").alias(ID_COL),\n",
    "              F.col(bucket_val_col).alias(\"bucket_value\"),\n",
    "              F.col(\"geo_bucket_res\").alias(\"bucket_res\"),\n",
    "              F.col(VEC_COL).alias(VEC_COL),\n",
    "              *(F.col(c) for c in [BAD_COL, NAN_COL, NORM_COL] if c in df.columns),\n",
    "          )\n",
    "          .limit(1)\n",
    "    )\n",
    "\n",
    "    target_row = _safe_head1(\n",
    "        target_df.select(\n",
    "            F.col(\"bucket_value\").alias(\"bucket_value\"),\n",
    "            F.col(\"bucket_res\").cast(\"int\").alias(\"bucket_res\"),\n",
    "            vector_to_array(F.col(VEC_COL)).alias(\"vec_arr\"),\n",
    "            (F.col(NORM_COL).cast(\"double\").alias(\"vec_norm\") if NORM_COL in df.columns else F.lit(None).alias(\"vec_norm\")),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    empty_neighbors = spark.createDataFrame(\n",
    "        [], schema=T.StructType([\n",
    "            T.StructField(ID_COL, T.StringType(), True),\n",
    "            T.StructField(\"similarity\", T.DoubleType(), True),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    if target_row is None:\n",
    "        return empty_neighbors, {\"target_property_id\": str(target_property_id), \"status\": \"target_not_found\"}\n",
    "\n",
    "    bucket_value = target_row[\"bucket_value\"]\n",
    "    bucket_res   = target_row[\"bucket_res\"]\n",
    "    target_arr   = target_row[\"vec_arr\"]\n",
    "    target_norm  = target_row[\"vec_norm\"]\n",
    "\n",
    "    if bucket_res is None:\n",
    "        raise ValueError(f\"Target property_id={target_property_id} has null geo_bucket_res.\")\n",
    "    if bucket_value is None:\n",
    "        raise ValueError(f\"Target property_id={target_property_id} has null {_pick_geo_col(df)} (bucket value).\")\n",
    "\n",
    "    h3_col = f\"h3_{int(bucket_res)}\"\n",
    "    if h3_col not in df.columns:\n",
    "        raise ValueError(f\"Expected column '{h3_col}' (derived from geo_bucket_res={bucket_res}) not found in df.\")\n",
    "\n",
    "    # Compute target norm if missing\n",
    "    if target_norm is None and target_arr is not None:\n",
    "        s = 0.0\n",
    "        for x in target_arr:\n",
    "            s += float(x) * float(x)\n",
    "        target_norm = s ** 0.5\n",
    "\n",
    "    if target_arr is None or target_norm is None or float(target_norm) == 0.0:\n",
    "        return empty_neighbors, {\n",
    "            \"target_property_id\": str(target_property_id),\n",
    "            \"status\": \"target_vector_invalid\",\n",
    "            \"bucket_res\": int(bucket_res),\n",
    "            \"bucket_value\": bucket_value,\n",
    "            \"h3_filter_column\": h3_col,\n",
    "        }\n",
    "\n",
    "    cand = (\n",
    "        df.filter(F.col(h3_col) == F.lit(bucket_value))\n",
    "          .filter(F.col(ID_COL).cast(\"string\") != F.lit(str(target_property_id)))\n",
    "          .filter(F.col(VEC_COL).isNotNull())\n",
    "    )\n",
    "\n",
    "    if BAD_COL in df.columns:\n",
    "        cand = cand.filter(~F.col(BAD_COL))\n",
    "    if NAN_COL in df.columns:\n",
    "        cand = cand.filter(~F.col(NAN_COL))\n",
    "    if NORM_COL in df.columns:\n",
    "        cand = cand.filter(F.col(NORM_COL).isNotNull() & (F.col(NORM_COL) > 0))\n",
    "\n",
    "    n_bucket = cand.count()\n",
    "    frac = min(1.0, (float(max_candidates) * float(oversample)) / max(float(n_bucket), 1.0))\n",
    "\n",
    "    cand = (\n",
    "        cand.sample(withReplacement=False, fraction=frac, seed=int(seed))\n",
    "            .limit(int(max_candidates))\n",
    "    )\n",
    "\n",
    "    target_arr_lit = F.array(*[F.lit(float(x)) for x in target_arr])\n",
    "    cand_arr = vector_to_array(F.col(VEC_COL))\n",
    "    cand = cand.withColumn(\"cand_arr\", cand_arr).withColumn(\"target_arr\", target_arr_lit)\n",
    "\n",
    "    dot = F.expr(\n",
    "        \"aggregate(\"\n",
    "        \"  arrays_zip(cand_arr, target_arr),\"\n",
    "        \"  cast(0.0 as double),\"\n",
    "        \"  (acc, x) -> acc + (x['cand_arr'] * x['target_arr'])\"\n",
    "        \")\"\n",
    "    )\n",
    "\n",
    "    if NORM_COL in df.columns:\n",
    "        cand_norm = F.col(NORM_COL).cast(\"double\")\n",
    "    else:\n",
    "        cand_norm = F.sqrt(\n",
    "            F.expr(\n",
    "                \"aggregate(\"\n",
    "                \"  transform(cand_arr, x -> x * x),\"\n",
    "                \"  cast(0.0 as double),\"\n",
    "                \"  (acc, y) -> acc + y\"\n",
    "                \")\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sim = (dot / (cand_norm * F.lit(float(target_norm)))).alias(\"similarity\")\n",
    "\n",
    "    neighbors_df = (\n",
    "        cand.select(F.col(ID_COL).cast(\"string\").alias(ID_COL), sim)\n",
    "            .orderBy(F.desc(\"similarity\"))\n",
    "            .limit(int(top_k))\n",
    "    )\n",
    "\n",
    "    meta = {\n",
    "        \"target_property_id\": str(target_property_id),\n",
    "        \"status\": \"ok\",\n",
    "        \"bucket_res\": int(bucket_res),\n",
    "        \"bucket_value\": bucket_value,\n",
    "        \"h3_filter_column\": h3_col,\n",
    "        \"bucket_candidate_count\": int(n_bucket),\n",
    "        \"candidate_sampling_fraction\": float(frac),\n",
    "        \"max_candidates_cap\": int(max_candidates),\n",
    "        \"oversample_factor\": float(oversample),\n",
    "        \"top_k_neighbors\": int(top_k),\n",
    "        \"random_seed\": int(seed),\n",
    "    }\n",
    "\n",
    "    return neighbors_df, meta\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) Spark NLP models (fit once)\n",
    "# =========================================================\n",
    "_SPLIT_MODEL = None\n",
    "_SENT_MODEL  = None\n",
    "_EMB_MODEL   = None\n",
    "_TOPICS_EMB_DF = None\n",
    "_TOPICS_EMB_MODEL_NAME = None\n",
    "\n",
    "def init_models(spark) -> None:\n",
    "    global _SPLIT_MODEL, _SENT_MODEL\n",
    "    if _SPLIT_MODEL is not None and _SENT_MODEL is not None:\n",
    "        return\n",
    "\n",
    "    dummy_reviews   = spark.createDataFrame([(\"x\", \"hello world. great stay!\")], [ID_COL, \"review_text\"])\n",
    "    dummy_sentences = spark.createDataFrame([(\"x\", \"great stay!\")], [ID_COL, \"sentence_text\"])\n",
    "\n",
    "    doc1 = DocumentAssembler().setInputCol(\"review_text\").setOutputCol(\"document\")\n",
    "    sent_detector = (\n",
    "        SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\", \"en\")\n",
    "        .setInputCols([\"document\"])\n",
    "        .setOutputCol(\"sentences\")\n",
    "    )\n",
    "    _SPLIT_MODEL = Pipeline(stages=[doc1, sent_detector]).fit(dummy_reviews)\n",
    "\n",
    "    doc2 = DocumentAssembler().setInputCol(\"sentence_text\").setOutputCol(\"document\")\n",
    "    tok2 = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "    sent2 = (\n",
    "        ViveknSentimentModel.pretrained()\n",
    "        .setInputCols([\"document\", \"token\"])\n",
    "        .setOutputCol(\"sentiment\")\n",
    "    )\n",
    "    _SENT_MODEL = Pipeline(stages=[doc2, tok2, sent2]).fit(dummy_sentences)\n",
    "\n",
    "def init_embedding_model(spark, use_model_name: str = \"tfhub_use\") -> None:\n",
    "    global _EMB_MODEL, _TOPICS_EMB_DF, _TOPICS_EMB_MODEL_NAME\n",
    "    if _EMB_MODEL is not None and _TOPICS_EMB_MODEL_NAME == use_model_name:\n",
    "        return\n",
    "\n",
    "    dummy = spark.createDataFrame([(\"x\", \"great stay\")], [ID_COL, \"sentence_text\"])\n",
    "\n",
    "    doc = DocumentAssembler().setInputCol(\"sentence_text\").setOutputCol(\"document\")\n",
    "    use = (\n",
    "        UniversalSentenceEncoder.pretrained(use_model_name, \"en\")\n",
    "        .setInputCols([\"document\"])\n",
    "        .setOutputCol(\"sentence_embeddings\")\n",
    "    )\n",
    "    _EMB_MODEL = Pipeline(stages=[doc, use]).fit(dummy)\n",
    "\n",
    "    _TOPICS_EMB_DF = None\n",
    "    _TOPICS_EMB_MODEL_NAME = use_model_name\n",
    "\n",
    "# =========================================================\n",
    "# 3) Sentence extraction + sentiment\n",
    "# =========================================================\n",
    "def build_sentences_with_sentiment(\n",
    "    df_subset: DataFrame,\n",
    "    *,\n",
    "    max_reviews_per_property: int = 50,\n",
    ") -> DataFrame:\n",
    "    if _SPLIT_MODEL is None or _SENT_MODEL is None:\n",
    "        raise RuntimeError(\"Models not initialized. Call init_models(spark) before running analysis.\")\n",
    "\n",
    "    df_fixed = df_subset.withColumn(\n",
    "        \"reviews_array\",\n",
    "        F.from_json(F.col(REV_COL), T.ArrayType(T.StringType()))\n",
    "    )\n",
    "\n",
    "    df_fixed = df_fixed.withColumn(\n",
    "        \"reviews_array\",\n",
    "        F.when(\n",
    "            F.col(\"reviews_array\").isNotNull(),\n",
    "            F.expr(f\"slice(reviews_array, 1, {int(max_reviews_per_property)})\")\n",
    "        ).otherwise(F.array())\n",
    "    )\n",
    "\n",
    "    df_reviews = (\n",
    "        df_fixed.select(\n",
    "            F.col(ID_COL).cast(\"string\").alias(ID_COL),\n",
    "            F.posexplode_outer(\"reviews_array\").alias(\"review_idx\", \"review_text\")\n",
    "        )\n",
    "        .filter(F.col(\"review_text\").isNotNull())\n",
    "        .withColumn(\"review_text\", F.regexp_replace(F.lower(F.col(\"review_text\")), r\"\\s+\", \" \"))\n",
    "    )\n",
    "\n",
    "    tmp = _SPLIT_MODEL.transform(df_reviews)\n",
    "\n",
    "    df_sentences = (\n",
    "        tmp.select(ID_COL, \"review_idx\", F.posexplode(\"sentences\").alias(\"sent_idx\", \"sent_annot\"))\n",
    "           .withColumn(\"sentence_text\", F.col(\"sent_annot.result\"))\n",
    "           .drop(\"sent_annot\")\n",
    "           .filter(F.col(\"sentence_text\").isNotNull() & (F.length(\"sentence_text\") > 0))\n",
    "    )\n",
    "\n",
    "    df_sentences = df_sentences.withColumn(\n",
    "        \"sentence_id\",\n",
    "        F.sha2(\n",
    "            F.concat_ws(\n",
    "                \"||\",\n",
    "                F.col(ID_COL),\n",
    "                F.col(\"review_idx\").cast(\"string\"),\n",
    "                F.col(\"sent_idx\").cast(\"string\"),\n",
    "                F.col(\"sentence_text\"),\n",
    "            ),\n",
    "            256,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    scored = _SENT_MODEL.transform(df_sentences)\n",
    "\n",
    "    return scored.select(\n",
    "        ID_COL,\n",
    "        \"review_idx\",\n",
    "        \"sent_idx\",\n",
    "        \"sentence_id\",\n",
    "        \"sentence_text\",\n",
    "        F.col(\"sentiment.result\").getItem(0).alias(\"sentiment_label\")\n",
    "    )\n",
    "\n",
    "# =========================================================\n",
    "# 4) Topic tagging via USE embeddings + cosine to topic vectors\n",
    "# =========================================================\n",
    "def _get_topics_embeddings(spark) -> DataFrame:\n",
    "    global _TOPICS_EMB_DF\n",
    "    if _TOPICS_EMB_DF is not None:\n",
    "        return _TOPICS_EMB_DF\n",
    "\n",
    "    if _EMB_MODEL is None:\n",
    "        raise RuntimeError(\"Embedding model not initialized. Call init_embedding_model(spark, ...)\")\n",
    "\n",
    "    topics_df = spark.createDataFrame([(t,) for t in TOPICS], [\"sentence_text\"])\n",
    "    te = _EMB_MODEL.transform(topics_df)\n",
    "\n",
    "    te = te.withColumn(\"topic_vector\", F.col(\"sentence_embeddings\")[0][\"embeddings\"]) \\\n",
    "           .drop(\"sentence_embeddings\", \"document\") \\\n",
    "           .withColumnRenamed(\"sentence_text\", \"topic\")\n",
    "\n",
    "    te = te.withColumn(\n",
    "        \"topic_vector_norm\",\n",
    "        F.sqrt(\n",
    "            F.expr(\n",
    "                \"aggregate(transform(topic_vector, x -> x * x), cast(0.0 as double), (acc, y) -> acc + y)\"\n",
    "            )\n",
    "        )\n",
    "    ).filter(F.col(\"topic_vector_norm\") > 0)\n",
    "\n",
    "    _TOPICS_EMB_DF = te.select(\"topic\", \"topic_vector\", \"topic_vector_norm\")\n",
    "    return _TOPICS_EMB_DF\n",
    "\n",
    "def classify_topics_with_use(\n",
    "    sent_df: DataFrame,\n",
    "    *,\n",
    "    similarity_threshold: float = 0.35,\n",
    "    top_topics_per_sentence: Optional[int] = 2,\n",
    "    non_ascii_ratio_max: float = 0.20,\n",
    "    topic_partitions: Optional[int] = None,\n",
    ") -> DataFrame:\n",
    "    if _EMB_MODEL is None:\n",
    "        raise RuntimeError(\"Embedding model not initialized. Call init_embedding_model(spark, ...)\")\n",
    "\n",
    "    spark = sent_df.sparkSession\n",
    "\n",
    "    base = sent_df.select(\n",
    "        F.col(ID_COL).cast(\"string\").alias(ID_COL),\n",
    "        \"sentence_id\",\n",
    "        \"sentence_text\"\n",
    "    )\n",
    "\n",
    "    base = filter_english_like(base, text_col=\"sentence_text\", non_ascii_ratio_max=float(non_ascii_ratio_max))\n",
    "\n",
    "    if topic_partitions is not None:\n",
    "        base = base.repartition(int(topic_partitions))\n",
    "\n",
    "    se = _EMB_MODEL.transform(base)\n",
    "\n",
    "    se = se.withColumn(\"sentence_vector\", F.col(\"sentence_embeddings\")[0][\"embeddings\"]) \\\n",
    "           .drop(\"sentence_embeddings\", \"document\")\n",
    "\n",
    "    se = se.withColumn(\n",
    "        \"sentence_vector_norm\",\n",
    "        F.sqrt(\n",
    "            F.expr(\n",
    "                \"aggregate(transform(sentence_vector, x -> x * x), cast(0.0 as double), (acc, y) -> acc + y)\"\n",
    "            )\n",
    "        )\n",
    "    ).filter(F.col(\"sentence_vector_norm\") > 0)\n",
    "\n",
    "    topics_emb = F.broadcast(_get_topics_embeddings(spark))\n",
    "    pairs = se.crossJoin(topics_emb)\n",
    "\n",
    "    dot = F.expr(\n",
    "        \"aggregate(\"\n",
    "        \"  arrays_zip(sentence_vector, topic_vector),\"\n",
    "        \"  cast(0.0 as double),\"\n",
    "        \"  (acc, x) -> acc + (x['sentence_vector'] * x['topic_vector'])\"\n",
    "        \")\"\n",
    "    )\n",
    "\n",
    "    scored = pairs.withColumn(\n",
    "        \"topic_similarity\",\n",
    "        dot / (F.col(\"sentence_vector_norm\") * F.col(\"topic_vector_norm\"))\n",
    "    ).filter(F.col(\"topic_similarity\") >= F.lit(float(similarity_threshold)))\n",
    "\n",
    "    if top_topics_per_sentence is not None:\n",
    "        w = Window.partitionBy(ID_COL, \"sentence_id\").orderBy(F.desc(\"topic_similarity\"))\n",
    "        scored = scored.withColumn(\"rn\", F.row_number().over(w)) \\\n",
    "                       .filter(F.col(\"rn\") <= int(top_topics_per_sentence)) \\\n",
    "                       .drop(\"rn\")\n",
    "\n",
    "    return scored.select(\n",
    "        ID_COL,\n",
    "        \"sentence_id\",\n",
    "        \"topic\",\n",
    "        F.col(\"topic_similarity\").cast(\"double\").alias(\"topic_similarity\")\n",
    "    )\n",
    "\n",
    "# =========================================================\n",
    "# 5) Report building (weaknesses + strengths + evidence)\n",
    "# =========================================================\n",
    "def _build_topic_report_df(\n",
    "    enriched: DataFrame,\n",
    "    *,\n",
    "    target_property_id: str,\n",
    "    top_k_weakness_topics: int,\n",
    "    top_k_strength_topics: int,\n",
    "    top_k_examples: int,\n",
    ") -> Tuple[DataFrame, DataFrame]:\n",
    "    my_df  = enriched.filter(F.col(ID_COL) == F.lit(str(target_property_id)))\n",
    "    nbr_df = enriched.filter(F.col(ID_COL) != F.lit(str(target_property_id)))\n",
    "\n",
    "    def agg(df: DataFrame, prefix: str) -> DataFrame:\n",
    "        return (\n",
    "            df.groupBy(\"topic\")\n",
    "              .agg(\n",
    "                  F.count(\"*\").alias(f\"{prefix}_sentence_topic_assignments\"),\n",
    "                  F.sum(F.when(F.col(\"sentiment_label\") == \"negative\", 1).otherwise(0)).alias(f\"{prefix}_negative_assignments\"),\n",
    "              )\n",
    "              .withColumn(\n",
    "                  f\"{prefix}_negative_rate\",\n",
    "                  F.when(\n",
    "                      F.col(f\"{prefix}_sentence_topic_assignments\") > 0,\n",
    "                      F.col(f\"{prefix}_negative_assignments\") / F.col(f\"{prefix}_sentence_topic_assignments\")\n",
    "                  ).otherwise(F.lit(0.0))\n",
    "              )\n",
    "        )\n",
    "\n",
    "    my_agg  = agg(my_df, \"target\")\n",
    "    nbr_agg = agg(nbr_df, \"neighbors\")\n",
    "\n",
    "    comp = (\n",
    "        my_agg.join(nbr_agg, on=\"topic\", how=\"outer\")\n",
    "              .na.fill(\n",
    "                  0,\n",
    "                  subset=[\n",
    "                      \"target_sentence_topic_assignments\",\n",
    "                      \"target_negative_assignments\",\n",
    "                      \"target_negative_rate\",\n",
    "                      \"neighbors_sentence_topic_assignments\",\n",
    "                      \"neighbors_negative_assignments\",\n",
    "                      \"neighbors_negative_rate\",\n",
    "                  ],\n",
    "              )\n",
    "              .withColumn(\"negative_rate_gap\", F.col(\"target_negative_rate\") - F.col(\"neighbors_negative_rate\"))\n",
    "    )\n",
    "\n",
    "    weaknesses_df = comp.orderBy(F.desc(\"negative_rate_gap\")).limit(int(top_k_weakness_topics))\n",
    "    strengths_df  = comp.orderBy(F.asc(\"negative_rate_gap\")).limit(int(top_k_strength_topics))\n",
    "\n",
    "    def attach_examples(df_topics: DataFrame) -> DataFrame:\n",
    "        topics_only = df_topics.select(\"topic\").distinct()\n",
    "\n",
    "        evidence_struct = F.struct(\n",
    "            F.col(ID_COL).alias(\"property_id\"),\n",
    "            F.col(\"review_idx\").cast(\"int\").alias(\"review_idx\"),\n",
    "            F.col(\"sent_idx\").cast(\"int\").alias(\"sent_idx\"),\n",
    "            F.col(\"sentence_id\").alias(\"sentence_id\"),\n",
    "            F.col(\"topic_similarity\").cast(\"double\").alias(\"topic_similarity\"),\n",
    "            F.col(\"sentiment_label\").alias(\"sentiment_label\"),\n",
    "            F.col(\"sentence_text\").alias(\"sentence_text\"),\n",
    "        )\n",
    "\n",
    "        w_t = Window.partitionBy(\"topic\").orderBy(F.desc(\"topic_similarity\"))\n",
    "        target_negative_examples = (\n",
    "            my_df.join(topics_only, on=\"topic\", how=\"inner\")\n",
    "                .filter(F.col(\"sentiment_label\") == \"negative\")\n",
    "                .withColumn(\"rn\", F.row_number().over(w_t))\n",
    "                .filter(F.col(\"rn\") <= int(top_k_examples))\n",
    "                .groupBy(\"topic\")\n",
    "                .agg(F.collect_list(evidence_struct).alias(\"target_negative_examples\"))\n",
    "        )\n",
    "\n",
    "        w_np = Window.partitionBy(\"topic\").orderBy(F.desc(\"topic_similarity\"))\n",
    "        neighbors_positive_examples = (\n",
    "            nbr_df.join(topics_only, on=\"topic\", how=\"inner\")\n",
    "                 .filter(F.col(\"sentiment_label\") == \"positive\")\n",
    "                 .withColumn(\"rn\", F.row_number().over(w_np))\n",
    "                 .filter(F.col(\"rn\") <= int(top_k_examples))\n",
    "                 .groupBy(\"topic\")\n",
    "                 .agg(F.collect_list(evidence_struct).alias(\"neighbors_positive_examples\"))\n",
    "        )\n",
    "\n",
    "        w_nn = Window.partitionBy(\"topic\").orderBy(F.desc(\"topic_similarity\"))\n",
    "        neighbors_negative_examples = (\n",
    "            nbr_df.join(topics_only, on=\"topic\", how=\"inner\")\n",
    "                 .filter(F.col(\"sentiment_label\") == \"negative\")\n",
    "                 .withColumn(\"rn\", F.row_number().over(w_nn))\n",
    "                 .filter(F.col(\"rn\") <= int(top_k_examples))\n",
    "                 .groupBy(\"topic\")\n",
    "                 .agg(F.collect_list(evidence_struct).alias(\"neighbors_negative_examples\"))\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            df_topics.join(target_negative_examples, on=\"topic\", how=\"left\")\n",
    "                    .join(neighbors_positive_examples, on=\"topic\", how=\"left\")\n",
    "                    .join(neighbors_negative_examples, on=\"topic\", how=\"left\")\n",
    "                    .withColumn(\"target_negative_examples\", F.coalesce(F.col(\"target_negative_examples\"), F.array()))\n",
    "                    .withColumn(\"neighbors_positive_examples\", F.coalesce(F.col(\"neighbors_positive_examples\"), F.array()))\n",
    "                    .withColumn(\"neighbors_negative_examples\", F.coalesce(F.col(\"neighbors_negative_examples\"), F.array()))\n",
    "        )\n",
    "\n",
    "    return attach_examples(weaknesses_df), attach_examples(strengths_df)\n",
    "\n",
    "def analyze_property_realtime_payload(\n",
    "    listings_df: DataFrame,\n",
    "    sim_df: DataFrame,\n",
    "    pid: str,\n",
    "    *,\n",
    "    similarity_threshold: float = 0.35,\n",
    "    top_topics_per_sentence: int = 2,\n",
    "    top_k_weakness_topics: int = 8,\n",
    "    top_k_strength_topics: Optional[int] = None,\n",
    "    top_k_examples: int = 3,\n",
    "    max_reviews_per_property: int = 50,\n",
    "    max_candidates: int = 50,\n",
    "    top_k_neighbors: int = 20,\n",
    "    seed: int = 42,\n",
    "    oversample: float = 1.7,\n",
    "    use_model_name: str = \"tfhub_use\",\n",
    "    non_ascii_ratio_max: float = 0.20,\n",
    "    topic_partitions: Optional[int] = None,\n",
    "    as_json_string: bool = True,\n",
    ") -> Any:\n",
    "    def _struct_list_to_dict_list(struct_list):\n",
    "        if not struct_list:\n",
    "            return []\n",
    "        out = []\n",
    "        for s in struct_list:\n",
    "            out.append({\n",
    "                \"property_id\": s[\"property_id\"],\n",
    "                \"review_idx\": int(s[\"review_idx\"]) if s[\"review_idx\"] is not None else None,\n",
    "                \"sent_idx\": int(s[\"sent_idx\"]) if s[\"sent_idx\"] is not None else None,\n",
    "                \"sentence_id\": s[\"sentence_id\"],\n",
    "                \"topic_similarity\": float(s[\"topic_similarity\"]) if s[\"topic_similarity\"] is not None else None,\n",
    "                \"sentiment_label\": s[\"sentiment_label\"],\n",
    "                \"sentence_text\": s[\"sentence_text\"],\n",
    "            })\n",
    "        return out\n",
    "\n",
    "    spark = listings_df.sparkSession\n",
    "\n",
    "    if top_k_strength_topics is None:\n",
    "        top_k_strength_topics = int(top_k_weakness_topics)\n",
    "\n",
    "    init_models(spark)\n",
    "    init_embedding_model(spark, use_model_name=use_model_name)\n",
    "\n",
    "    neighbors_df, neighbor_meta = top_similar_properties_with_metadata(\n",
    "        df=sim_df,\n",
    "        target_property_id=pid,\n",
    "        max_candidates=max_candidates,\n",
    "        top_k=top_k_neighbors,\n",
    "        seed=seed,\n",
    "        oversample=oversample,\n",
    "    )\n",
    "\n",
    "    neighbors_rows = neighbors_df.orderBy(F.desc(\"similarity\")).collect()\n",
    "    neighbor_ids: List[str] = [r[ID_COL] for r in neighbors_rows]\n",
    "    neighbor_sims: List[float] = [float(r[\"similarity\"]) if r[\"similarity\"] is not None else None for r in neighbors_rows]\n",
    "\n",
    "    sim_vals = [s for s in neighbor_sims if s is not None]\n",
    "    neighbor_similarity_stats = {\n",
    "        \"neighbors_selected_count\": int(len(neighbor_ids)),\n",
    "        \"similarity_avg\": float(sum(sim_vals) / len(sim_vals)) if sim_vals else None,\n",
    "        \"similarity_min\": float(min(sim_vals)) if sim_vals else None,\n",
    "        \"similarity_max\": float(max(sim_vals)) if sim_vals else None,\n",
    "    }\n",
    "\n",
    "    neighbor_list = [\n",
    "        {\"neighbor_property_id\": nid, \"similarity\": float(s) if s is not None else None}\n",
    "        for nid, s in zip(neighbor_ids, neighbor_sims)\n",
    "    ]\n",
    "\n",
    "    ids_df = (\n",
    "        spark.createDataFrame([(str(pid),)], [ID_COL])\n",
    "             .unionByName(neighbors_df.select(ID_COL))\n",
    "             .distinct()\n",
    "    )\n",
    "\n",
    "    df_subset = (\n",
    "        listings_df.select(F.col(ID_COL).cast(\"string\").alias(ID_COL), F.col(REV_COL))\n",
    "                   .join(ids_df, on=ID_COL, how=\"inner\")\n",
    "    )\n",
    "\n",
    "    sent_df = build_sentences_with_sentiment(\n",
    "        df_subset=df_subset,\n",
    "        max_reviews_per_property=max_reviews_per_property\n",
    "    ).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    sent_counts = sent_df.groupBy(ID_COL).agg(F.countDistinct(\"sentence_id\").alias(\"distinct_sentences\"))\n",
    "    target_sentence_count_row = _safe_head1(sent_counts.filter(F.col(ID_COL) == F.lit(str(pid))))\n",
    "    target_distinct_sentences = int(target_sentence_count_row[\"distinct_sentences\"]) if target_sentence_count_row else 0\n",
    "    neighbors_distinct_sentences = int(\n",
    "        sent_counts.filter(F.col(ID_COL) != F.lit(str(pid)))\n",
    "                   .agg(F.sum(\"distinct_sentences\").alias(\"s\"))\n",
    "                   .first()[\"s\"] or 0\n",
    "    )\n",
    "\n",
    "    topics_df = classify_topics_with_use(\n",
    "        sent_df=sent_df,\n",
    "        similarity_threshold=similarity_threshold,\n",
    "        top_topics_per_sentence=top_topics_per_sentence,\n",
    "        non_ascii_ratio_max=non_ascii_ratio_max,\n",
    "        topic_partitions=topic_partitions,\n",
    "    ).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    enriched = (\n",
    "        sent_df.join(topics_df, on=[ID_COL, \"sentence_id\"], how=\"inner\")\n",
    "              .select(\n",
    "                  F.col(ID_COL).cast(\"string\").alias(ID_COL),\n",
    "                  F.col(\"review_idx\").cast(\"int\").alias(\"review_idx\"),\n",
    "                  F.col(\"sent_idx\").cast(\"int\").alias(\"sent_idx\"),\n",
    "                  F.col(\"sentence_id\").alias(\"sentence_id\"),\n",
    "                  F.col(\"topic\").alias(\"topic\"),\n",
    "                  F.col(\"topic_similarity\").cast(\"double\").alias(\"topic_similarity\"),\n",
    "                  F.col(\"sentiment_label\").alias(\"sentiment_label\"),\n",
    "                  F.col(\"sentence_text\").alias(\"sentence_text\"),\n",
    "              )\n",
    "              .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    )\n",
    "\n",
    "    weaknesses_df, strengths_df = _build_topic_report_df(\n",
    "        enriched=enriched,\n",
    "        target_property_id=str(pid),\n",
    "        top_k_weakness_topics=int(top_k_weakness_topics),\n",
    "        top_k_strength_topics=int(top_k_strength_topics),\n",
    "        top_k_examples=int(top_k_examples),\n",
    "    )\n",
    "\n",
    "    weaknesses = []\n",
    "    for r in weaknesses_df.collect():\n",
    "        weaknesses.append({\n",
    "            \"topic\": r[\"topic\"],\n",
    "            \"target_sentence_topic_assignments\": int(r[\"target_sentence_topic_assignments\"]),\n",
    "            \"target_negative_assignments\": int(r[\"target_negative_assignments\"]),\n",
    "            \"target_negative_rate\": float(r[\"target_negative_rate\"]),\n",
    "            \"neighbors_sentence_topic_assignments\": int(r[\"neighbors_sentence_topic_assignments\"]),\n",
    "            \"neighbors_negative_assignments\": int(r[\"neighbors_negative_assignments\"]),\n",
    "            \"neighbors_negative_rate\": float(r[\"neighbors_negative_rate\"]),\n",
    "            \"negative_rate_gap\": float(r[\"negative_rate_gap\"]),\n",
    "            \"target_negative_examples\": _struct_list_to_dict_list(r[\"target_negative_examples\"]),\n",
    "            \"neighbors_positive_examples\": _struct_list_to_dict_list(r[\"neighbors_positive_examples\"]),\n",
    "            \"neighbors_negative_examples\": _struct_list_to_dict_list(r[\"neighbors_negative_examples\"]),\n",
    "        })\n",
    "\n",
    "    strengths = []\n",
    "    for r in strengths_df.collect():\n",
    "        strengths.append({\n",
    "            \"topic\": r[\"topic\"],\n",
    "            \"target_sentence_topic_assignments\": int(r[\"target_sentence_topic_assignments\"]),\n",
    "            \"target_negative_assignments\": int(r[\"target_negative_assignments\"]),\n",
    "            \"target_negative_rate\": float(r[\"target_negative_rate\"]),\n",
    "            \"neighbors_sentence_topic_assignments\": int(r[\"neighbors_sentence_topic_assignments\"]),\n",
    "            \"neighbors_negative_assignments\": int(r[\"neighbors_negative_assignments\"]),\n",
    "            \"neighbors_negative_rate\": float(r[\"neighbors_negative_rate\"]),\n",
    "            \"negative_rate_gap\": float(r[\"negative_rate_gap\"]),\n",
    "            \"target_negative_examples\": _struct_list_to_dict_list(r[\"target_negative_examples\"]),\n",
    "            \"neighbors_positive_examples\": _struct_list_to_dict_list(r[\"neighbors_positive_examples\"]),\n",
    "            \"neighbors_negative_examples\": _struct_list_to_dict_list(r[\"neighbors_negative_examples\"]),\n",
    "        })\n",
    "\n",
    "    payload: Dict[str, Any] = {\n",
    "        \"target_property_id\": str(pid),\n",
    "        \"neighbors\": {\n",
    "            \"neighbor_list\": neighbor_list,\n",
    "            \"similarity_stats\": neighbor_similarity_stats,\n",
    "            \"selection_metadata\": neighbor_meta,\n",
    "        },\n",
    "        \"text_volume\": {\n",
    "            \"target_distinct_sentences_used\": int(target_distinct_sentences),\n",
    "            \"neighbors_distinct_sentences_used\": int(neighbors_distinct_sentences),\n",
    "            \"max_reviews_per_property_cap\": int(max_reviews_per_property),\n",
    "        },\n",
    "        \"topic_tagging_config\": {\n",
    "            \"embedding_model\": str(use_model_name),\n",
    "            \"similarity_threshold\": float(similarity_threshold),\n",
    "            \"top_topics_per_sentence\": int(top_topics_per_sentence),\n",
    "            \"non_ascii_ratio_max\": float(non_ascii_ratio_max),\n",
    "        },\n",
    "        \"results\": {\n",
    "            \"weaknesses_vs_neighbors\": weaknesses,\n",
    "            \"strengths_vs_neighbors\": strengths,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # cleanup\n",
    "    sent_df.unpersist()\n",
    "    topics_df.unpersist()\n",
    "    enriched.unpersist()\n",
    "\n",
    "    return json.dumps(payload, ensure_ascii=False) if as_json_string else payload\n",
    "\n",
    "# =========================================================\n",
    "# 6) Write payload to Delta output paths\n",
    "# =========================================================\n",
    "def write_payload_to_delta_paths(\n",
    "    *,\n",
    "    spark: SparkSession,\n",
    "    payload: dict,\n",
    "    out_base_path: str,\n",
    "    run_id: str,\n",
    ") -> dict:\n",
    "    if not out_base_path:\n",
    "        raise ValueError(\"out_base_path is required\")\n",
    "\n",
    "    ts = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "    pid = payload.get(\"target_property_id\")\n",
    "\n",
    "    runs_path      = out_base_path.rstrip(\"/\") + \"/runs\"\n",
    "    neighbors_path = out_base_path.rstrip(\"/\") + \"/neighbors\"\n",
    "    topics_path    = out_base_path.rstrip(\"/\") + \"/topics\"\n",
    "    evidence_path  = out_base_path.rstrip(\"/\") + \"/evidence\"\n",
    "\n",
    "    # Runs row (store payload_json for debugging; you can drop later)\n",
    "    runs_row = {\n",
    "        \"run_id\": run_id,\n",
    "        \"run_utc_ts\": ts,\n",
    "        \"property_id\": pid,\n",
    "        \"status\": \"ok\",\n",
    "        \"neighbors_selected_count\": payload.get(\"neighbors\", {}).get(\"similarity_stats\", {}).get(\"neighbors_selected_count\"),\n",
    "        \"target_distinct_sentences_used\": payload.get(\"text_volume\", {}).get(\"target_distinct_sentences_used\"),\n",
    "        \"neighbors_distinct_sentences_used\": payload.get(\"text_volume\", {}).get(\"neighbors_distinct_sentences_used\"),\n",
    "        \"payload_json\": json.dumps(payload, ensure_ascii=False),\n",
    "    }\n",
    "    spark.createDataFrame([runs_row]).write.format(\"delta\").mode(\"append\").save(runs_path)\n",
    "\n",
    "    # Neighbors\n",
    "    neighbor_list = payload.get(\"neighbors\", {}).get(\"neighbor_list\", []) or []\n",
    "    neighbors_rows = []\n",
    "    for x in neighbor_list:\n",
    "        neighbors_rows.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"run_utc_ts\": ts,\n",
    "            \"property_id\": pid,\n",
    "            \"neighbor_property_id\": x.get(\"neighbor_property_id\"),\n",
    "            \"similarity\": x.get(\"similarity\"),\n",
    "        })\n",
    "    if neighbors_rows:\n",
    "        spark.createDataFrame(neighbors_rows).write.format(\"delta\").mode(\"append\").save(neighbors_path)\n",
    "\n",
    "    # Topics\n",
    "    topics_rows = []\n",
    "    for kind, arr in [\n",
    "        (\"weakness\", payload.get(\"results\", {}).get(\"weaknesses_vs_neighbors\", []) or []),\n",
    "        (\"strength\", payload.get(\"results\", {}).get(\"strengths_vs_neighbors\", []) or []),\n",
    "    ]:\n",
    "        for t in arr:\n",
    "            topics_rows.append({\n",
    "                \"run_id\": run_id,\n",
    "                \"run_utc_ts\": ts,\n",
    "                \"property_id\": pid,\n",
    "                \"kind\": kind,\n",
    "                \"topic\": t.get(\"topic\"),\n",
    "                \"negative_rate_gap\": t.get(\"negative_rate_gap\"),\n",
    "                \"target_negative_rate\": t.get(\"target_negative_rate\"),\n",
    "                \"neighbors_negative_rate\": t.get(\"neighbors_negative_rate\"),\n",
    "                \"target_sentence_topic_assignments\": t.get(\"target_sentence_topic_assignments\"),\n",
    "                \"neighbors_sentence_topic_assignments\": t.get(\"neighbors_sentence_topic_assignments\"),\n",
    "            })\n",
    "    if topics_rows:\n",
    "        spark.createDataFrame(topics_rows).write.format(\"delta\").mode(\"append\").save(topics_path)\n",
    "\n",
    "    # Evidence (bounded text)\n",
    "    def _push_evidence(kind: str, topic: str, evidence_arr: list, role: str):\n",
    "        rows = []\n",
    "        for e in (evidence_arr or []):\n",
    "            txt = e.get(\"sentence_text\")\n",
    "            if txt and len(txt) > 600:\n",
    "                txt = txt[:600] + \"â€¦\"\n",
    "            rows.append({\n",
    "                \"run_id\": run_id,\n",
    "                \"run_utc_ts\": ts,\n",
    "                \"property_id\": pid,\n",
    "                \"kind\": kind,\n",
    "                \"topic\": topic,\n",
    "                \"evidence_role\": role,\n",
    "                \"evidence_property_id\": e.get(\"property_id\"),\n",
    "                \"review_idx\": e.get(\"review_idx\"),\n",
    "                \"sent_idx\": e.get(\"sent_idx\"),\n",
    "                \"sentence_id\": e.get(\"sentence_id\"),\n",
    "                \"topic_similarity\": e.get(\"topic_similarity\"),\n",
    "                \"sentiment_label\": e.get(\"sentiment_label\"),\n",
    "                \"sentence_text\": txt,\n",
    "            })\n",
    "        return rows\n",
    "\n",
    "    evidence_rows = []\n",
    "    for kind, arr in [\n",
    "        (\"weakness\", payload.get(\"results\", {}).get(\"weaknesses_vs_neighbors\", []) or []),\n",
    "        (\"strength\", payload.get(\"results\", {}).get(\"strengths_vs_neighbors\", []) or []),\n",
    "    ]:\n",
    "        for t in arr:\n",
    "            topic = t.get(\"topic\")\n",
    "            evidence_rows += _push_evidence(kind, topic, t.get(\"target_negative_examples\"), \"target_negative\")\n",
    "            evidence_rows += _push_evidence(kind, topic, t.get(\"neighbors_positive_examples\"), \"neighbors_positive\")\n",
    "            evidence_rows += _push_evidence(kind, topic, t.get(\"neighbors_negative_examples\"), \"neighbors_negative\")\n",
    "\n",
    "    if evidence_rows:\n",
    "        spark.createDataFrame(evidence_rows).write.format(\"delta\").mode(\"append\").save(evidence_path)\n",
    "\n",
    "    return {\n",
    "        \"runs_path\": runs_path,\n",
    "        \"neighbors_path\": neighbors_path,\n",
    "        \"topics_path\": topics_path,\n",
    "        \"evidence_path\": evidence_path,\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# 7) Job main: load data, run analysis, write outputs, exit JSON\n",
    "# =========================================================\n",
    "DEFAULT_PARAMS = dict(\n",
    "    similarity_threshold=0.35,\n",
    "    top_topics_per_sentence=2,\n",
    "    top_k_weakness_topics=8,\n",
    "    top_k_strength_topics=8,\n",
    "    top_k_examples=3,\n",
    "    max_reviews_per_property=50,\n",
    "    max_candidates=100,\n",
    "    top_k_neighbors=20,\n",
    "    seed=42,\n",
    "    oversample=1.7,\n",
    "    use_model_name=\"tfhub_use\",\n",
    "    non_ascii_ratio_max=0.20,\n",
    "    topic_partitions=None,\n",
    ")\n",
    "\n",
    "def _parse_params_json(s: str) -> dict:\n",
    "    if not s:\n",
    "        return {}\n",
    "    obj = json.loads(s)\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(\"params_json must decode to a JSON object (dict).\")\n",
    "    return obj\n",
    "\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "run_id = str(uuid.uuid4())\n",
    "ts = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "try:\n",
    "    user_params = _parse_params_json(PARAMS_JSON)\n",
    "    params = {**DEFAULT_PARAMS, **user_params}\n",
    "\n",
    "    airbnb_df = spark.read.format(\"delta\").load(DELTA_PATH)\n",
    "    sim_df = airbnb_df\n",
    "\n",
    "    payload = analyze_property_realtime_payload(\n",
    "        listings_df=airbnb_df,\n",
    "        sim_df=sim_df,\n",
    "        pid=str(PID),\n",
    "        as_json_string=False,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    written = write_payload_to_delta_paths(\n",
    "        spark=spark,\n",
    "        payload=payload,\n",
    "        out_base_path=OUT_BASE_PATH,\n",
    "        run_id=run_id,\n",
    "    )\n",
    "\n",
    "    dbutils.notebook.exit(json.dumps({\n",
    "        \"status\": \"ok\",\n",
    "        \"run_id\": run_id,\n",
    "        \"run_utc_ts\": ts,\n",
    "        \"property_id\": str(PID),\n",
    "        \"written\": written,\n",
    "        \"params_used\": params,\n",
    "    }, ensure_ascii=False))\n",
    "\n",
    "except Exception as e:\n",
    "    # Best-effort: if out_base_path is provided, write an error run row so Flask can see failures via SQL\n",
    "    err_msg = str(e)\n",
    "    try:\n",
    "        runs_path = OUT_BASE_PATH.rstrip(\"/\") + \"/runs\"\n",
    "        spark.createDataFrame([{\n",
    "            \"run_id\": run_id,\n",
    "            \"run_utc_ts\": ts,\n",
    "            \"property_id\": str(PID),\n",
    "            \"status\": \"error\",\n",
    "            \"error_message\": err_msg[:2000],\n",
    "            \"payload_json\": None,\n",
    "            \"neighbors_selected_count\": None,\n",
    "            \"target_distinct_sentences_used\": None,\n",
    "            \"neighbors_distinct_sentences_used\": None,\n",
    "        }]).write.format(\"delta\").mode(\"append\").save(runs_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    dbutils.notebook.exit(json.dumps({\n",
    "        \"status\": \"error\",\n",
    "        \"run_id\": run_id,\n",
    "        \"run_utc_ts\": ts,\n",
    "        \"property_id\": str(PID),\n",
    "        \"error_message\": err_msg[:4000],\n",
    "    }, ensure_ascii=False))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NLP Tool",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}